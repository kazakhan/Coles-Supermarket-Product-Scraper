{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b97d44-fc46-4f89-b7ba-58e3f3f726f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "\n",
    "def configure_driver():\n",
    "    \"\"\"\n",
    "    Configures and initializes the Edge driver.\n",
    "    \n",
    "    Returns:\n",
    "        WebDriver: Initialized Edge WebDriver instance.\n",
    "    \"\"\"\n",
    "    driver = webdriver.Edge()\n",
    "    return driver\n",
    "\n",
    "def get_categories(driver, url):\n",
    "    \"\"\"\n",
    "    Retrieves product categories from the Coles website.\n",
    "    \n",
    "    Args:\n",
    "        driver (WebDriver): Initialized WebDriver instance.\n",
    "        url (str): URL of the Coles website.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of BeautifulSoup elements representing categories.\n",
    "    \"\"\"\n",
    "    driver.get(url + \"/browse\")\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    categories = soup.find_all(\"a\", class_=\"coles-targeting-ShopCategoriesShopCategoryStyledCategoryContainer\")\n",
    "    for category in categories:\n",
    "        print(category.text)\n",
    "    return categories\n",
    "\n",
    "def scrape_products_in_category(driver, category, url):\n",
    "    \"\"\"\n",
    "    Scrapes products within a given category and writes them to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        driver (WebDriver): Initialized WebDriver instance.\n",
    "        category (BeautifulSoup): BeautifulSoup element representing a category.\n",
    "        url (str): Base URL of the Coles website.\n",
    "    \"\"\"\n",
    "    category_link = category.get(\"href\")\n",
    "    if category_link == \"/browse/tobacco\":\n",
    "        return\n",
    "    category_link = url + category_link\n",
    "    print(category_link)\n",
    "    driver.get(category_link)\n",
    "    \n",
    "    while True:\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        products = soup.find_all(\"header\", class_=\"product__header\")\n",
    "        filename = category.text + \".csv\"\n",
    "        filepath = os.path.join(\"D:\\\\Documents\\\\Budget\", filename)\n",
    "        \n",
    "        with open(filepath, \"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            \n",
    "            for product in products:\n",
    "                name = product.find(\"h2\", class_=\"product__title\")\n",
    "                price = product.find(\"span\", class_=\"price__value\")\n",
    "                product_link = product.find(\"a\", class_=\"product__link\")[\"href\"]\n",
    "                product_code = product_link.split(\"-\")[-1]\n",
    "                if name and price:\n",
    "                    name = name.text.strip()\n",
    "                    price = price.text.strip()\n",
    "                    link = url + product_link\n",
    "                    writer.writerow([product_code, name, price, link])\n",
    "                    \n",
    "            pagination = soup.find(\"ul\", class_=\"coles-targeting-PaginationPaginationUl\")\n",
    "            if not pagination:\n",
    "                break\n",
    "            if pagination:\n",
    "                pages = pagination.find_all(\"li\")\n",
    "                last_page = int(pages[-2].text.strip()) if pages else 1\n",
    "            else:\n",
    "                last_page = 1\n",
    "            total_pages = int(pages[-2].text.strip())\n",
    "            print(total_pages)            \n",
    "            for page in range(2, last_page + 1):\n",
    "                next_page_link = f\"{category_link}?page={page}\"\n",
    "                driver.get(next_page_link)\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                products = soup.find_all(\"header\", class_=\"product__header\")\n",
    "                \n",
    "                for product in products:\n",
    "                    name = product.find(\"h2\", class_=\"product__title\")\n",
    "                    price = product.find(\"span\", class_=\"price__value\")\n",
    "                    product_link = product.find(\"a\", class_=\"product__link\")[\"href\"]\n",
    "                    product_code = product_link.split(\"-\")[-1]\n",
    "                    if name and price:\n",
    "                        name = name.text.strip()\n",
    "                        price = price.text.strip()\n",
    "                        link = url + product_link\n",
    "                        writer.writerow([product_code, name, price, link])\n",
    "                time.sleep(random.randint(1, 6))     \n",
    "            if page == last_page:\n",
    "                break\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the scraping process.\n",
    "    \"\"\"\n",
    "    url = \"https://www.coles.com.au\"\n",
    "    options = Options()\n",
    "    driver = configure_driver()\n",
    "    \n",
    "    try:\n",
    "        print(\"Here we go...\")\n",
    "        categories = get_categories(driver, url)\n",
    "        \n",
    "        for category in categories:\n",
    "            scrape_products_in_category(driver, category, url)\n",
    "            time.sleep(random.randint(1, 6))\n",
    "            \n",
    "        print(\"Finished\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add67fca-09e0-46ce-a3c7-a9a21fb956ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import sqlite3\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "\n",
    "def configure_driver():\n",
    "    \"\"\"\n",
    "    Configures and initializes the Edge driver.\n",
    "    \n",
    "    Returns:\n",
    "        WebDriver: Initialized Edge WebDriver instance.\n",
    "    \"\"\"\n",
    "    driver = webdriver.Edge()\n",
    "    return driver\n",
    "\n",
    "def get_categories(driver, url):\n",
    "    \"\"\"\n",
    "    Retrieves product categories from the Coles website.\n",
    "    \n",
    "    Args:\n",
    "        driver (WebDriver): Initialized WebDriver instance.\n",
    "        url (str): URL of the Coles website.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of BeautifulSoup elements representing categories.\n",
    "    \"\"\"\n",
    "    driver.get(url + \"/browse\")\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    categories = soup.find_all(\"a\", class_=\"coles-targeting-ShopCategoriesShopCategoryStyledCategoryContainer\")\n",
    "    for category in categories:\n",
    "        print(category.text)\n",
    "    return categories\n",
    "\n",
    "def create_database():\n",
    "    \"\"\"\n",
    "    Creates a SQLite database to store product information.\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the SQLite database file.\n",
    "    \"\"\"\n",
    "    db_path = \"products.db\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # Create products table if it doesn't exist\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS products (\n",
    "                    product_code INTEGER PRIMARY KEY,\n",
    "                    category TEXT,\n",
    "                    name TEXT,\n",
    "                    price TEXT,\n",
    "                    link TEXT,\n",
    "                    on_special BOOLEAN DEFAULT 0\n",
    "                 )''')\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    return db_path\n",
    "\n",
    "def insert_product(conn, category, product_code, name, price, link, on_special=False):\n",
    "    \"\"\"\n",
    "    Inserts or updates a product into the SQLite database.\n",
    "    \n",
    "    Args:\n",
    "        conn (sqlite3.Connection): SQLite database connection.\n",
    "        category (str): Product category.\n",
    "        product_code (str): Product code.\n",
    "        name (str): Product name.\n",
    "        price (str): Product price.\n",
    "        link (str): Product link.\n",
    "        on_special (bool): Whether the product is on special (default is False).\n",
    "    \"\"\"\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # Check if the product_code already exists\n",
    "    c.execute(\"SELECT * FROM products WHERE product_code=?\", (product_code,))\n",
    "    existing_product = c.fetchone()\n",
    "    \n",
    "    if existing_product:\n",
    "        # Update the existing row\n",
    "        c.execute(\"UPDATE products SET category=?, name=?, price=?, link=?, on_special=? WHERE product_code=?\",\n",
    "                  (category, name, price, link, on_special, product_code))\n",
    "    else:\n",
    "        # Insert a new row\n",
    "        c.execute(\"INSERT INTO products (category, product_code, name, price, link, on_special) VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "                  (category, product_code, name, price, link, on_special))\n",
    "    \n",
    "    conn.commit()\n",
    "\n",
    "def scrape_products_in_category(driver, category, url, conn):\n",
    "    \"\"\"\n",
    "    Scrapes products within a given category and inserts them into the SQLite database.\n",
    "    \n",
    "    Args:\n",
    "        driver (WebDriver): Initialized WebDriver instance.\n",
    "        category (BeautifulSoup): BeautifulSoup element representing a category.\n",
    "        url (str): Base URL of the Coles website.\n",
    "        conn (sqlite3.Connection): SQLite database connection.\n",
    "    \"\"\"\n",
    "    category_name = category.text.strip()\n",
    "    category_link = category.get(\"href\")\n",
    "    if category_link == \"/browse/tobacco\":\n",
    "        return\n",
    "    category_link = url + category_link\n",
    "    print(category_link)\n",
    "    driver.get(category_link)\n",
    "    \n",
    "    while True:\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        products = soup.find_all(\"header\", class_=\"product__header\")\n",
    "        \n",
    "        for product in products:\n",
    "            name = product.find(\"h2\", class_=\"product__title\")\n",
    "            price = product.find(\"span\", class_=\"price__value\")\n",
    "            product_link = product.find(\"a\", class_=\"product__link\")[\"href\"]\n",
    "            product_code = product_link.split(\"-\")[-1]\n",
    "            if name and price:\n",
    "                name = name.text.strip()\n",
    "                price = price.text.strip()\n",
    "                link = url + product_link\n",
    "                insert_product(conn, category_name, product_code, name, price, link)\n",
    "                \n",
    "        pagination = soup.find(\"ul\", class_=\"coles-targeting-PaginationPaginationUl\")\n",
    "        if not pagination:\n",
    "            break\n",
    "        if pagination:\n",
    "            pages = pagination.find_all(\"li\")\n",
    "            last_page = int(pages[-2].text.strip()) if pages else 1\n",
    "        else:\n",
    "            last_page = 1\n",
    "        total_pages = int(pages[-2].text.strip())\n",
    "        print(total_pages)            \n",
    "        for page in range(2, last_page + 1):\n",
    "            next_page_link = f\"{category_link}?page={page}\"\n",
    "            driver.get(next_page_link)\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            products = soup.find_all(\"header\", class_=\"product__header\")\n",
    "            \n",
    "            for product in products:\n",
    "                name = product.find(\"h2\", class_=\"product__title\")\n",
    "                price = product.find(\"span\", class_=\"price__value\")\n",
    "                product_link = product.find(\"a\", class_=\"product__link\")[\"href\"]\n",
    "                product_code = product_link.split(\"-\")[-1]\n",
    "                if name and price:\n",
    "                    name = name.text.strip()\n",
    "                    price = price.text.strip()\n",
    "                    link = url + product_link\n",
    "                    special = 0;\n",
    "                    if category_name == \"Specials\":\n",
    "                        special = 1;\n",
    "                    insert_product(conn, category_name, product_code, name, price, link, special)\n",
    "            time.sleep(random.randint(1, 5))     \n",
    "        if page == last_page:\n",
    "            break\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the scraping process.\n",
    "    \"\"\"\n",
    "    url = \"https://www.coles.com.au\"\n",
    "    options = Options()\n",
    "    driver = configure_driver()\n",
    "    db_path = create_database()\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    try:\n",
    "        print(\"Here we go...\")\n",
    "        categories = get_categories(driver, url)\n",
    "        \n",
    "        for category in categories:\n",
    "            scrape_products_in_category(driver, category, url, conn)\n",
    "            time.sleep(random.randint(1, 5))\n",
    "            \n",
    "        print(\"Finished\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
